{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving a DAN trained on clean data with ASR Confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook reads (clean and ASR) QuizBowl data using a file called \"dataset.py\" and builds a Deep Averaging Network that leverages confidences to improve over the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies - helpful Python tools, PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general python\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from typing import List, Optional, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook \n",
    "from collections import defaultdict\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#torchtext.  Dataset is a custom file for handling QuizBowl data\n",
    "from dataset import QuizBowl\n",
    "from torchtext.data.field import Field\n",
    "from torchtext.data.iterator import Iterator\n",
    "\n",
    "#code to read-in torchtext data and optional way to refresh those modules\n",
    "import dataset\n",
    "import dataset_confidence\n",
    "#import importlib\n",
    "#importlib.reload(dataset_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contains code that designs a Deep Averaging Network, variations thereupon in ascending order of complexity, and classes that learn confidence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegression(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        \n",
    "        super(LogRegression, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        self.text_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.265)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "                \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.dropout(averaged)\n",
    "            return self.classifier(averaged_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to the logistic regression above, the DAN adds in a hidden layer of 1000 units towards the end the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        #initialize the vocab from the text field (passed in from train_iter) and pad\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        \n",
    "        #run the vocab through Glove Embeddings\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "        \n",
    "        #set the unknown items to the mean embedding and make them cuda()\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        self.text_embeddings.weight.requires_grad = False \n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        #the classifier converts the hidden dimensions into the answers.  \n",
    "        #It takes batch norm and dropout as well.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "\n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        #if the text exists, run it through embeddings, pool, dropout, and then run it through a hidden layer\n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnormed_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnormed_dropped)\n",
    "            return self.classifier(nonlinear )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below Confidence Learner is used to learn mappings from embeddings and confidences to a single value.  This is used in the modified DAN that follows. The simple confidence learner simply passes on the confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfidenceLearner(nn.Module):\n",
    "    def __init__(self, embeddings_dim, confidences_dim):\n",
    "        super(ConfidenceLearner, self).__init__()\n",
    "        self.transform = nn.Linear((embeddings_dim + confidences_dim), 1)\n",
    "    \n",
    "    def forward(self, embeds, confs):\n",
    "        concat = torch.cat((embeds, confs), -1)\n",
    "        return torch.sigmoid(self.transform(concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConfidenceLearner(nn.Module):\n",
    "    def __init__(self, embeddings_dim, confidences_dim):\n",
    "        #does nothing\n",
    "        super(SimpleConfidenceLearner, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, embeds, confs):\n",
    "        return confs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DAN ingests a \"confidence\" field from TorchText, which is a vector of confidences in range 0-1. Both init and forward are adjusted to accomodate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN_Confidences(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_Confidences, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #confidences are learned from word_embeddings and respective word_confidence\n",
    "        self.confidences = ConfidenceLearner(embedding_dim, 1)\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),  #make this h1_dim+1 when appeneded\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            confidences = Variable(confidences).cuda()\n",
    "            confidences = self.confidences(embed, confidences)\n",
    "            multiplied = embed * confidences\n",
    "            averaged = self._pool(multiplied, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnormed_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnormed_dropped)\n",
    "            return self.classifier(nonlinear )\n",
    "        \n",
    "        \n",
    "        #layer.weights.data[:, -1] = pretrainedweights\n",
    "        #layer.bias.data[: -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN_Confidences_Softmax(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_Confidences_Softmax, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim+1, answer_size),  #make this h1_dim+1 when appeneded\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            #do this elsewhere\n",
    "            confidences = Variable(confidences).cuda()                   \n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)  \n",
    "            batchnorm_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnorm_dropped)\n",
    "            expanded = torch.cat((nonlinear,confidences.mean(dim=1).unsqueeze(-1)), 1)\n",
    "            return self.classifier(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# pretrainweights = model.classifier[0].weights.data\n",
    "# load from existing model\n",
    "\n",
    "#in the new model, in __init__\n",
    "#layer.weights.data[:, :-1] = pretrainedweights\n",
    "#layer.bias.data[:-1] #this is vector\n",
    "\n",
    "#for average, just give it existing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN_WeightedMean(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_WeightedMean, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        #self.confidences = ConfidenceLearner(embedding_dim, 1) # ADDED IN FOR VARIATION\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self,  embed, lengths, confidences):\n",
    "        embed = embed * confidences.unsqueeze(2).expand_as(embed) \n",
    "        embed = embed.sum(1)\n",
    "        return embed / confidences.sum(dim = 1).unsqueeze(1).expand_as(embed)\n",
    "                    \n",
    "        #for learning variation\n",
    "              #pass in output of CONFIDENCE LEARNER into POOOl\n",
    "              #dimensions will be the same, just learning new value for confidence\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "                \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            confidences = Variable(confidences).cuda() \n",
    "            #confidences = self.confidences(embed, confidences).squeeze() #ADDED IN FOR VARIATION\n",
    "            averaged = self._pool(embed, lengths['text'].float(), confidences)\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnorm_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnorm_dropped)\n",
    "            return self.classifier(batchnorm_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses a custom class from dataset.py to read in the data into TorchText, which allows for easy batching in PyTorch.  The second cell extracts the vocab and answers for this particular model, to use as mappings for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "train_iter, val_iter, dev_iter = dataset.QuizBowl.iters(\n",
    "            batch_size=512,\n",
    "            lower= True,\n",
    "            use_wiki=False,  #irrelevant\n",
    "            n_wiki_sentences=5, #irrelevant \n",
    "            replace_title_mentions='',\n",
    "            combined_ngrams=True,\n",
    "            unigrams=True, \n",
    "            bigrams=False, #irrelevant \n",
    "            trigrams=False, #irrelevant \n",
    "            combined_max_vocab_size=300000,\n",
    "            unigram_max_vocab_size= None, \n",
    "            bigram_max_vocab_size=50000, #irrelevant \n",
    "            trigram_max_vocab_size=50000 #irrelevant \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields: Dict[str, Field] = train_iter.dataset.fields\n",
    "page_field = fields['page']\n",
    "clean_answer_mappings = page_field.vocab.stoi\n",
    "clean_vocab_mappings = fields['text'].vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section does the exact same as above, only for ASR data (rather than clean data).  Dataset_confidence is a modified version of dataset that ingests a \"confidence\" field, has different formatting for the text (tupples rather than full sentences), and does not shuffle as data has been shuffled up front. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_asr, val_iter_asr, dev_iter_asr = dataset_confidence.QuizBowl.iters(\n",
    "            batch_size=512,\n",
    "            lower= True,\n",
    "            use_wiki=False,  #irrelevant\n",
    "            n_wiki_sentences=5, #irrelevant \n",
    "            replace_title_mentions='',\n",
    "            combined_ngrams=True,\n",
    "            unigrams=True, \n",
    "            bigrams=False, #irrelevant \n",
    "            trigrams=False, #irrelevant \n",
    "            combined_max_vocab_size=300000,\n",
    "            unigram_max_vocab_size= None, \n",
    "            bigram_max_vocab_size=50000, #irrelevant \n",
    "            trigram_max_vocab_size=50000 #irrelevant \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields: Dict[str, Field] = train_iter_asr.dataset.fields\n",
    "page_field_asr = fields['page']\n",
    "asr_answer_mappings = page_field_asr.vocab.stoi\n",
    "asr_vocab_mappings = fields['text'].vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates default dictionaries that converts VOCAB and ANSWERS from ASR to Clean torch datatsets and vice versa.  Code redundant for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_to_asr = defaultdict(int)\n",
    "count = 0\n",
    "for key, vals in clean_answer_mappings.items():\n",
    "    if key in asr_answer_mappings.keys():\n",
    "        clean_to_asr[vals] = asr_answer_mappings[key]\n",
    "clean_to_asr[0] = 0 #many possible options for 0, so ensure that this gets mapped to 0\n",
    "\n",
    "asr_to_clean = defaultdict(int)\n",
    "for key, vals in asr_answer_mappings.items():\n",
    "    if key in clean_answer_mappings.keys():\n",
    "        asr_to_clean[vals] = clean_answer_mappings[key]     \n",
    "#asr_to_clean = {v: k for k, v in clean_to_asr.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_to_asr_vocab = defaultdict(int)\n",
    "count = 0\n",
    "for key, vals in clean_vocab_mappings.items():\n",
    "    if key in asr_vocab_mappings.keys():\n",
    "        clean_to_asr_vocab[vals] = asr_vocab_mappings[key]\n",
    "clean_to_asr_vocab[0] = 0\n",
    "\n",
    "asr_to_clean_vocab = defaultdict(int)\n",
    "for key, vals in asr_vocab_mappings.items():\n",
    "    if key in clean_vocab_mappings.keys():\n",
    "        asr_to_clean_vocab[vals] = clean_vocab_mappings[key]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Running Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loops through batches, runs them through the network, calculates loss, and backpropogates.  It can both train and simply run through the network (for testing).  The code marked as \"IMPORTANT\" passes the code through the ASR_to_Clean dictionary that is necessary for train/testing to work across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(iterator: Iterator):\n",
    "        is_train = iterator.train\n",
    "        batch_accuracies = []\n",
    "        batch_losses = []\n",
    "        epoch_start = time.time()\n",
    "        for batch in iterator:\n",
    "            input_dict = {}\n",
    "            lengths_dict = {}\n",
    "            if hasattr(batch, 'text'):\n",
    "                text, lengths = batch.text\n",
    "                \n",
    "                #IMPORTANT.  ADJUSTS BETWEEN DATASETS\n",
    "                text = text.data.cpu().numpy().tolist()\n",
    "                for i, group in enumerate(text):\n",
    "                    #print(group)\n",
    "                    for j, item in enumerate(group):\n",
    "                        text[i][j]=asr_to_clean_vocab[item]\n",
    "                text = Variable(torch.from_numpy(np.asarray(text))).cuda()\n",
    "  \n",
    "                input_dict['text'] = text\n",
    "                lengths_dict['text'] = lengths\n",
    "            page = batch.page\n",
    "            qnums = batch.qnum.cuda()\n",
    "\n",
    "            if is_train:\n",
    "                model.zero_grad()\n",
    "\n",
    "            out = model(input_dict, lengths_dict, qnums)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            \n",
    "           #IMPORTANT.  ADJUSTS BETWEEN DATASETS\n",
    "            page = page.data.cpu().numpy()\n",
    "            page = [asr_to_clean[i] for i in page]\n",
    "            page = Variable(torch.from_numpy(np.asarray(page))).cuda()\n",
    "            \n",
    "            \n",
    "            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]\n",
    "            batch_loss = loss_function(out, page)\n",
    "            if is_train:\n",
    "                batch_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), .25)\n",
    "                optimizer.step()\n",
    "\n",
    "            batch_accuracies.append(accuracy)\n",
    "            batch_losses.append(batch_loss.data[0])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a variation on the base version that handles confidences: confidences = batch.confidence.  If confidences are not present, it creates a tensor that matches the size of the text, and populates it with all 1s.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch_confidences(iterator: Iterator):\n",
    "        is_train = iterator.train\n",
    "        batch_accuracies = []\n",
    "        batch_losses = []\n",
    "        epoch_start = time.time()\n",
    "        for batch in iterator:\n",
    "            input_dict = {}\n",
    "            lengths_dict = {}\n",
    "            if hasattr(batch, 'text'):\n",
    "                text, lengths = batch.text\n",
    "                \n",
    "                #IMPORTANT.  ADJUSTS BETWEEN DATASETS\n",
    "                text = text.data.cpu().numpy().tolist()\n",
    "                for i, group in enumerate(text):\n",
    "                    for j, item in enumerate(group):\n",
    "                        text[i][j]=asr_to_clean_vocab[item]\n",
    "                text = Variable(torch.from_numpy(np.asarray(text))).cuda()\n",
    "                \n",
    "                input_dict['text'] = text\n",
    "                lengths_dict['text'] = lengths\n",
    "\n",
    "            page = batch.page\n",
    "            \n",
    "            #IMPORTANT.  ADJUSTS BETWEEN DATASETS\n",
    "            page = page.data.cpu().numpy()\n",
    "            page = [asr_to_clean[i] for i in page]\n",
    "            page = Variable(torch.from_numpy(np.asarray(page))).cuda()\n",
    "            \n",
    "            \n",
    "            qnums = batch.qnum.cuda()\n",
    "            if hasattr(batch, 'confidence'):\n",
    "                confidences = batch.confidence\n",
    "            else:\n",
    "                confidences = torch.FloatTensor(batch.text[0].shape).fill_(1)\n",
    "                #print (\"Missing confidences\")\n",
    "                #raise\n",
    "            \n",
    "            if is_train:\n",
    "                model.zero_grad()\n",
    "\n",
    "            out = model(input_dict, lengths_dict, qnums, confidences,)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]\n",
    "            batch_loss = loss_function(out, page)\n",
    "            if is_train:\n",
    "                batch_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), .25)\n",
    "                optimizer.step()\n",
    "\n",
    "            batch_accuracies.append(accuracy)\n",
    "            batch_losses.append(batch_loss.data[0])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the dimensions and epochs for model\n",
    "EMBEDDING_DIM = 300 \n",
    "HIDDEN_DIM = 1000\n",
    "EPOCH = 70\n",
    "\n",
    "#extract fields to determine vocabulary size of answers\n",
    "fields: Dict[str, Field] = train_iter.dataset.fields\n",
    "page_field = fields['page']\n",
    "ANSWER_SIZE = len(page_field.vocab.stoi)\n",
    " \n",
    "model =DAN_WeightedMean(EMBEDDING_DIM,\n",
    "             HIDDEN_DIM,\n",
    "             fields['text'], \n",
    "             ANSWER_SIZE)\n",
    "model = model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables to generate graphs, save best model\n",
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "best_accuracy = 0.\n",
    "\n",
    "for i in tqdm_notebook(range(20)):  \n",
    "    #train\n",
    "    model.train()\n",
    "    train_acc, train_loss, train_time = run_epoch_confidences(train_iter_asr)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    #validate\n",
    "    model.eval()\n",
    "    val_acc, val_loss, val_time = run_epoch_confidences(val_iter_asr)    \n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        #torch.save(model.state_dict(), './cleanmodel.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test on final.  Dev is taken from the ASR data, and not from Clean data, \n",
    "#so this can be dramatically lower than validation accuracy\n",
    "model.eval()\n",
    "test_acc, test_loss, test_time = run_epoch_confidences(dev_iter_asr)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Log Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.plot(val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
