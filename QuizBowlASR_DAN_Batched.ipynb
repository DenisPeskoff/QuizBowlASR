{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving a DAN trained on clean data with ASR Confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook reads (clean and ASR) QuizBowl data using a file called \"dataset.py\" and builds a Deep Averaging Network that leverages confidences to improve over the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies - helpful Python tools, PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7b6e5e6b6af2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "#general python\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from typing import List, Optional, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook \n",
    "from collections import defaultdict\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#torchtext.  Dataset is a custom file for handling QuizBowl data\n",
    "from dataset import QuizBowl\n",
    "from torchtext.data.field import Field\n",
    "from torchtext.data.iterator import Iterator\n",
    "\n",
    "#code to read-in torchtext data and optional way to refresh those modules\n",
    "import dataset\n",
    "import dataset_confidence\n",
    "#import importlib\n",
    "#importlib.reload(dataset_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contains code that designs a Deep Averaging Network, variations thereupon in ascending order of complexity, and classes that learn confidence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegression(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        \n",
    "        super(LogRegression, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        self.text_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.265)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "                \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.dropout(averaged)\n",
    "            return self.classifier(averaged_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to the logistic regression above, the DAN adds in a hidden layer of 1000 units towards the end the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        #initialize the vocab from the text field (passed in from train_iter) and pad\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        \n",
    "        #run the vocab through Glove Embeddings\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "        \n",
    "        #set the unknown items to the mean embedding and make them cuda()\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        #self.text_embeddings.weight.requires_grad = False \n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        #the classifier converts the hidden dimensions into the answers.  \n",
    "        #It takes batch norm and dropout as well.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "\n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        #if the text exists, run it through embeddings, pool, dropout, and then run it through a hidden layer\n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnormed_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnormed_dropped)\n",
    "            return self.classifier(nonlinear )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        #initialize the vocab from the text field (passed in from train_iter) and pad\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        \n",
    "        #run the vocab through Glove Embeddings\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "        \n",
    "        #set the unknown items to the mean embedding and make them cuda()\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        #self.text_embeddings.weight.requires_grad = False \n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        self.rnn = nn.RNN(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        #the classifier converts the hidden dimensions into the answers.  \n",
    "        #It takes batch norm and dropout as well.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"\n",
    "        Return variables that we can use as h_0 and c_0. \n",
    "        \"\"\"\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        #if the text exists, run it through embeddings, pool, dropout, and then run it through a hidden layer\n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            x, self.hidden = self.rnn(x.view(len(x), 1, -1), self.hidden)\n",
    "            nonlinear = self.nonlinear(x)\n",
    "            return self.classifier(x)\n",
    "            \n",
    "            #averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            #averaged_dropped = self.small_dropout(averaged)\n",
    "            #hidden_layer = self.hidden(averaged_dropped)\n",
    "            #batchnormed_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below Confidence Learner is used to learn mappings from embeddings and confidences to a single value.  This is used in the modified DAN that follows. The simple confidence learner simply passes on the confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConfidenceLearner(nn.Module):\n",
    "    def __init__(self, embeddings_dim, confidences_dim):\n",
    "        super(ConfidenceLearner, self).__init__()\n",
    "        self.transform = nn.Linear((embeddings_dim + confidences_dim), 1)\n",
    "    \n",
    "    def forward(self, embeds, confs):\n",
    "        concat = torch.cat((embeds, confs), -1)\n",
    "        data = torch.sigmoid(self.transform(concat))\n",
    "        return data\n",
    "    \n",
    "    \n",
    "#train with SimpleConfidenceLearner.  Then freeze embeddings and train ConfidenceLearner.\n",
    "#loop through each parameter and set weights equal to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleConfidenceLearner(nn.Module):\n",
    "    def __init__(self, embeddings_dim, confidences_dim):\n",
    "        #does nothing\n",
    "        super(SimpleConfidenceLearner, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, embeds, confs):\n",
    "        return confs.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DAN ingests a \"confidence\" field from TorchText, which is a vector of confidences in range 0-1. Both init and forward are adjusted to accomodate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DAN_Confidences(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_Confidences, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #confidences are learned from word_embeddings and respective word_confidence\n",
    "        self.confidences = SimpleConfidenceLearner(embedding_dim, 1)\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),  #make this h1_dim+1 when appeneded\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            confidences = Variable(confidences).cuda()\n",
    "            confidences = self.confidences(embed, confidences)\n",
    "            multiplied = embed * confidences\n",
    "            averaged = self._pool(multiplied, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnormed_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnormed_dropped)\n",
    "            return self.classifier(nonlinear )\n",
    "        \n",
    "        \n",
    "        #layer.weights.data[:, -1] = pretrainedweights\n",
    "        #layer.bias.data[: -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DAN_Confidences_Softmax(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_Confidences_Softmax, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        #self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim+1, answer_size),  #make this h1_dim+1 when appeneded\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            #do this elsewhere\n",
    "            confidences = Variable(confidences).cuda()                   \n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)  \n",
    "            batchnorm_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnorm_dropped)\n",
    "            expanded = torch.cat((nonlinear,confidences.mean(dim=1).unsqueeze(-1)), 1)\n",
    "            return self.classifier(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# pretrainweights = model.classifier[0].weights.data\n",
    "# load from existing model\n",
    "\n",
    "#in the new model, in __init__\n",
    "#layer.weights.data[:, :-1] = pretrainedweights\n",
    "#layer.bias.data[:-1] #this is vector\n",
    "\n",
    "#for average, just give it existing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DAN_WeightedMean(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_WeightedMean, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        #self.confidences = ConfidenceLearner(embedding_dim, 1) # ADDED IN FOR VARIATION\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self,  embed, lengths, confidences):\n",
    "        embed = embed * confidences.unsqueeze(2).expand_as(embed) \n",
    "        embed = embed.sum(1)\n",
    "        return embed / confidences.sum(dim = 1).unsqueeze(1).expand_as(embed)\n",
    "                    \n",
    "        #for learning variation\n",
    "              #pass in output of CONFIDENCE LEARNER into POOOl\n",
    "              #dimensions will be the same, just learning new value for confidence\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "                \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            confidences = Variable(confidences).cuda() \n",
    "            #confidences = self.confidences(embed, confidences).squeeze() #ADDED IN FOR VARIATION\n",
    "            averaged = self._pool(embed, lengths['text'].float(), confidences)\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnorm_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnorm_dropped)\n",
    "            return self.classifier(batchnorm_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses a custom class from dataset.py to read in the data into TorchText, which allows for easy batching in PyTorch.  The second cell extracts the vocab and answers for this particular model, to use as mappings for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "train_iter, val_iter, dev_iter = dataset.QuizBowl.iters(\n",
    "            batch_size=512,\n",
    "            lower= True,\n",
    "            use_wiki=False,  #irrelevant\n",
    "            n_wiki_sentences=5, #irrelevant \n",
    "            replace_title_mentions='',\n",
    "            combined_ngrams=True,\n",
    "            unigrams=True, \n",
    "            bigrams=False, #irrelevant \n",
    "            trigrams=False, #irrelevant \n",
    "            combined_max_vocab_size=300000,\n",
    "            unigram_max_vocab_size= None, \n",
    "            bigram_max_vocab_size=50000, #irrelevant \n",
    "            trigram_max_vocab_size=50000 #irrelevant \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields: Dict[str, Field] = train_iter.dataset.fields\n",
    "page_field = fields['page']\n",
    "clean_answer_mappings = page_field.vocab.stoi\n",
    "clean_vocab_mappings = fields['text'].vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL.  For verifying clean/clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "train_iter_buzzer, val_iter_buzzer, dev_iter_buzzer_same = dataset.QuizBowl.iters(\n",
    "            batch_size=512,\n",
    "            lower= True,\n",
    "            use_wiki=False,  #irrelevant\n",
    "            n_wiki_sentences=5, #irrelevant \n",
    "            replace_title_mentions='',\n",
    "            combined_ngrams=True,\n",
    "            unigrams=True, \n",
    "            bigrams=False, #irrelevant \n",
    "            trigrams=False, #irrelevant \n",
    "            combined_max_vocab_size=300000,\n",
    "            unigram_max_vocab_size= None, \n",
    "            bigram_max_vocab_size=50000, #irrelevant \n",
    "            trigram_max_vocab_size=50000 #irrelevant \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields: Dict[str, Field] = train_iter_buzzer.dataset.fields\n",
    "page_field = fields['page']\n",
    "buzz_answer_mappings = page_field.vocab.stoi\n",
    "buzz_vocab_mappings = fields['text'].vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section does the exact same as above, only for ASR data (rather than clean data).  Dataset_confidence is a modified version of dataset that ingests a \"confidence\" field, has different formatting for the text (tupples rather than full sentences), and does not shuffle as data has been shuffled up front. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset_confidence)\n",
    "train_iter_asr, val_iter_asr, dev_iter_asr = dataset_confidence.QuizBowl.iters(\n",
    "            batch_size=512,\n",
    "            lower= True,\n",
    "            use_wiki=False,  #irrelevant\n",
    "            n_wiki_sentences=5, #irrelevant \n",
    "            replace_title_mentions='',\n",
    "            combined_ngrams=True,\n",
    "            unigrams=True, \n",
    "            bigrams=False, #irrelevant \n",
    "            trigrams=False, #irrelevant \n",
    "            combined_max_vocab_size=300000,\n",
    "            unigram_max_vocab_size= None, \n",
    "            bigram_max_vocab_size=50000, #irrelevant \n",
    "            trigram_max_vocab_size=50000 #irrelevant \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields: Dict[str, Field] = train_iter_asr.dataset.fields\n",
    "page_field_asr = fields['page']\n",
    "asr_answer_mappings = page_field_asr.vocab.stoi\n",
    "asr_vocab_mappings = fields['text'].vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates default dictionaries that converts VOCAB and ANSWERS from ASR to Clean torch datatsets and vice versa.  Code redundant for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_to_asr = defaultdict(int)\n",
    "count = 0\n",
    "for key, vals in clean_answer_mappings.items():\n",
    "    if key in asr_answer_mappings.keys():\n",
    "        clean_to_asr[vals] = asr_answer_mappings[key]\n",
    "clean_to_asr[0] = 0 #many possible options for 0, so ensure that this gets mapped to 0\n",
    "\n",
    "asr_to_clean = defaultdict(int)\n",
    "for key, vals in asr_answer_mappings.items():\n",
    "    if key in clean_answer_mappings.keys():\n",
    "        asr_to_clean[vals] = clean_answer_mappings[key]     \n",
    "#asr_to_clean = {v: k for k, v in clean_to_asr.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_to_asr_vocab = defaultdict(int)\n",
    "count = 0\n",
    "for key, vals in clean_vocab_mappings.items():\n",
    "    if key in asr_vocab_mappings.keys():\n",
    "        clean_to_asr_vocab[vals] = asr_vocab_mappings[key]\n",
    "clean_to_asr_vocab[0] = 0\n",
    "\n",
    "asr_to_clean_vocab = defaultdict(int)\n",
    "for key, vals in asr_vocab_mappings.items():\n",
    "    if key in clean_vocab_mappings.keys():\n",
    "        asr_to_clean_vocab[vals] = clean_vocab_mappings[key]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### OPTIONAL FOR CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_to_buzz = defaultdict(int)\n",
    "count = 0\n",
    "for key, vals in clean_answer_mappings.items():\n",
    "    if key in buzz_answer_mappings.keys():\n",
    "        clean_to_buzz[vals] = buzz_answer_mappings[key]\n",
    "clean_to_buzz[0] = 0 #many possible options for 0, so ensure that this gets mapped to 0\n",
    "\n",
    "buzz_to_clean = defaultdict(int)\n",
    "for key, vals in buzz_answer_mappings.items():\n",
    "    if key in clean_answer_mappings.keys():\n",
    "        buzz_to_clean[vals] = clean_answer_mappings[key]     \n",
    "#asr_to_clean = {v: k for k, v in clean_to_asr.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_to_buzz_vocab = defaultdict(int)\n",
    "count = 0\n",
    "for key, vals in clean_vocab_mappings.items():\n",
    "    if key in buzz_vocab_mappings.keys():\n",
    "        clean_to_buzz_vocab[vals] = buzz_vocab_mappings[key]\n",
    "clean_to_buzz_vocab[0] = 0\n",
    "\n",
    "buzz_to_clean_vocab = defaultdict(int)\n",
    "for key, vals in buzz_vocab_mappings.items():\n",
    "    if key in clean_vocab_mappings.keys():\n",
    "        buzz_to_clean_vocab[vals] = clean_vocab_mappings[key]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Running Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loops through batches, runs them through the network, calculates loss, and backpropogates.  It can both train and simply run through the network (for testing).  The code marked as \"IMPORTANT\" passes the code through the ASR_to_Clean dictionary that is necessary for train/testing to work across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(iterator: Iterator):\n",
    "        is_train = iterator.train\n",
    "        batch_accuracies = []\n",
    "        batch_losses = []\n",
    "        epoch_start = time.time()\n",
    "        for batch in iterator:\n",
    "            input_dict = {}\n",
    "            lengths_dict = {}\n",
    "            if hasattr(batch, 'text'):\n",
    "                text, lengths = batch.text\n",
    "                \n",
    "                #IMPORTANT.  ADJUSTS BETWEEN DATASETS\n",
    "                text = text.data.cpu().numpy().tolist()\n",
    "                for i, group in enumerate(text):\n",
    "                    #print(group)\n",
    "                    for j, item in enumerate(group):\n",
    "                        text[i][j]=asr_to_clean_vocab[item]\n",
    "                text = Variable(torch.from_numpy(np.asarray(text))).cuda()\n",
    "  \n",
    "                input_dict['text'] = text\n",
    "                lengths_dict['text'] = lengths\n",
    "            page = batch.page\n",
    "            qnums = batch.qnum.cuda()\n",
    "\n",
    "            if is_train:\n",
    "                model.zero_grad()\n",
    "\n",
    "            out = model(input_dict, lengths_dict, qnums)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            \n",
    "           #IMPORTANT.  ADJUSTS BETWEEN DATASETS\n",
    "            page = page.data.cpu().numpy()\n",
    "            page = [asr_to_clean[i] for i in page]\n",
    "            page = Variable(torch.from_numpy(np.asarray(page))).cuda()\n",
    "            \n",
    "            \n",
    "            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]\n",
    "            batch_loss = loss_function(out, page)\n",
    "            if is_train:\n",
    "                batch_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), .25)\n",
    "                optimizer.step()\n",
    "\n",
    "            batch_accuracies.append(accuracy)\n",
    "            batch_losses.append(batch_loss.data[0])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a variation on the base version that handles confidences: confidences = batch.confidence.  If confidences are not present, it creates a tensor that matches the size of the text, and populates it with all 1s.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch_confidences(iterator: Iterator):\n",
    "        is_train = iterator.train\n",
    "        batch_accuracies = []\n",
    "        batch_losses = []\n",
    "        epoch_start = time.time()\n",
    "        for batch in iterator:\n",
    "            input_dict = {}\n",
    "            lengths_dict = {}\n",
    "            if hasattr(batch, 'text'):\n",
    "                text, lengths = batch.text\n",
    "                \n",
    "                #IMPORTANT.  ADJUSTS BETWEEN DATASETS\n",
    "#                 text = text.data.cpu().numpy().tolist()\n",
    "#                 for i, group in enumerate(text):\n",
    "#                     for j, item in enumerate(group):\n",
    "#                         text[i][j]=asr_to_clean_vocab[item]\n",
    "#                 text = Variable(torch.from_numpy(np.asarray(text))).cuda()\n",
    "                \n",
    "                input_dict['text'] = text\n",
    "                lengths_dict['text'] = lengths\n",
    "\n",
    "            page = batch.page\n",
    "            \n",
    "            #IMPORTANT.  ADJUSTS BETWEEN DATASETS\n",
    "#             page = page.data.cpu().numpy()\n",
    "#             page = [asr_to_clean[i] for i in page]\n",
    "#             page = Variable(torch.from_numpy(np.asarray(page))).cuda()\n",
    "            \n",
    "            \n",
    "            qnums = batch.qnum.cuda()\n",
    "            #if hasattr(batch, 'confidence'):\n",
    "            confidences = batch.confidence\n",
    "            #else:\n",
    "            #    confidences = torch.FloatTensor(batch.text[0].shape).fill_(1)\n",
    "                #print (\"Missing confidences\")\n",
    "                #raise\n",
    "            \n",
    "            if is_train:\n",
    "                model.zero_grad()\n",
    "\n",
    "            out = model(input_dict, lengths_dict, qnums, confidences,)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]\n",
    "            batch_loss = loss_function(out, page)\n",
    "            if is_train:\n",
    "                batch_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), .25)\n",
    "                optimizer.step()\n",
    "\n",
    "            batch_accuracies.append(accuracy)\n",
    "            batch_losses.append(batch_loss.data[0])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the dimensions and epochs for model\n",
    "EMBEDDING_DIM = 300 \n",
    "HIDDEN_DIM = 1000\n",
    "EPOCH = 25\n",
    "\n",
    "#extract fields to determine vocabulary size of answers\n",
    "#UPDATE THIS BETWEEN CLEAN AND ASR\n",
    "fields: Dict[str, Field] = train_iter_asr.dataset.fields\n",
    "page_field = fields['page']\n",
    "ANSWER_SIZE = len(page_field.vocab.stoi)\n",
    " \n",
    "model =DAN_WeightedMean(EMBEDDING_DIM,\n",
    "             HIDDEN_DIM,\n",
    "             fields['text'], \n",
    "             ANSWER_SIZE)\n",
    "model = model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fcb61818b342838e8f12c300b2ee21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cliphomes/dpeskov/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#variables to generate graphs, save best model\n",
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "best_accuracy = 0.\n",
    "last_accuracy = 0\n",
    "\n",
    "for i in tqdm_notebook(range(EPOCH)):  \n",
    "    #train\n",
    "    model.train()\n",
    "    train_acc, train_loss, train_time = run_epoch_confidences(train_iter_asr)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    #validate\n",
    "    model.eval()\n",
    "    val_acc, val_loss, val_time = run_epoch_confidences(val_iter_asr)    \n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    #if val_acc < last_accuracy:\n",
    "        #break\n",
    "        #raise (\"Stopping early at Epoch \", i)\n",
    "    \n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        #torch.save(model.state_dict(), './cleanmodel.pth')\n",
    "        \n",
    "    last_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cliphomes/dpeskov/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1617440951722009"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test on final.  Dev is taken from the ASR data, and not from Clean data, \n",
    "#so this can be dramatically lower than validation accuracy\n",
    "model.eval()\n",
    "test_acc, test_loss, test_time = run_epoch_confidences(dev_iter_asr)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11938138610937378,\n",
       " 0.1202295304022052,\n",
       " 0.12049586562947794,\n",
       " 0.1197175019505349,\n",
       " 0.1201708929782564,\n",
       " 0.12027391960675066,\n",
       " 0.11975714174861257,\n",
       " 0.1187409394505349,\n",
       " 0.12127587368542497,\n",
       " 0.12132501229643822,\n",
       " 0.1210586770691655,\n",
       " 0.12231582301584157,\n",
       " 0.12061003561724316,\n",
       " 0.1221826554022052,\n",
       " 0.12254251878369939,\n",
       " 0.12351908128369939,\n",
       " 0.12312432784925807,\n",
       " 0.12261704863472418,\n",
       " 0.12291188063946637,\n",
       " 0.11880432645028288,\n",
       " 0.12190682136199692,\n",
       " 0.12249812957915393,\n",
       " 0.1220938769931143,\n",
       " 0.12054025483402339,\n",
       " 0.12143753807653081]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff8ddd660f0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff8ddcf2e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Dan Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f309eb05940>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30f4b5c4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ASR]",
   "language": "python",
   "name": "conda-env-ASR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
