{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuizBowl ASR Transcription Generation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file generates text-to-speech MP3s, converts those MP3s into a specific .wav format, then decodes the .wav files with Kaldi.  This file has the below dependencies.  Additionally it pulls in QuizBowl data from a pre-generated JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#audio\n",
    "from gtts import gTTS\n",
    "import subprocess\n",
    "\n",
    "#storage and displays\n",
    "import json\n",
    "from tqdm import tqdm_notebook \n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first paragraph loads JSON.  The second paragraph selects the relevant part of the JSON from only the buzzer fold.  The last paragraph maps each sentence in a full QuizBowl question (generally 4-5 per question) to the respective answer.  Total data size ends up being ~111k sentences representing ~26k questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('quiz-bowl.all.json') as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "buzzer_data = []\n",
    "for question in data['questions']:\n",
    "    if question['fold'] == 'buzzertrain':\n",
    "        buzzer_data.append([question['sentences'], question['page'], question['qnum']])\n",
    "\n",
    "data = []\n",
    "for index, sentences in enumerate([x[0] for x in buzzer_data]):\n",
    "        for sent_count, sentence in enumerate(sentences):\n",
    "            data.append([buzzer_data[index][2], sent_count, sentence, buzzer_data[index][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate  Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates mp3 files from the data generated above.  Takes ~18 hours on all 110k sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for sentence in tqdm_notebook(data, total = len(data)):\n",
    "    file_name = str(sentence[0]) + \"_\" + str(sentence[1])\n",
    "    text = sentence[2]\n",
    "    #convert into audio with gTTS, save it to mp3, convert it to WAV\n",
    "    sentTTS = gTTS(text, lang='en', slow=False)\n",
    "    sentTTS.save('buzzer/'+file_name+\".mp3\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaldi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first paragraph sorts the files by numerical order (1_1, 1_2, 2_1, 2_2) rather than (1_0, 1_1, 10_0, 10_1).  The second paragraph loops through the files, generates a .wav file for Kaldi, and then runs online2-wav-nnet3-latgen-faster on it, recording both the transcription and the lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5269276a60c047aeae39ea0104fafaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "#sort list orthographically rather than lexigraphically\n",
    "file_list = []\n",
    "for each_file in (glob.glob('buzzer/*.mp3')):\n",
    "    file_list.append(each_file)\n",
    "file_list.sort(key = lambda file: int(file[file.find('/')+1:file.find('_')]) )\n",
    "\n",
    "\n",
    "transcriptions = {}\n",
    "error_files = []\n",
    "for each_file in tqdm_notebook(file_list, total=len(file_list)):\n",
    "    file_name = each_file [each_file.find('/')+1:each_file.find('.')]\n",
    "    file_name_mp3 = file_name+ \".mp3\"\n",
    "    file_name_wav = \"wav/\"+file_name+\".wav\"\n",
    "    #file_name_edit = \"wav_edit/\"+file_name+\"_edit.wav\"\n",
    "    file_name_lat = \"lattices/\"+file_name+\".lat\"\n",
    "    \n",
    "    #convert mp3 to a specific Kaldi-friendly wav\n",
    "    hide_output_variable = !ffmpeg -y -i {each_file} -acodec pcm_s16le -ac 1 -ar 8000 {file_name_wav}\n",
    "   \n",
    "    #Kaldi\n",
    "    try:\n",
    "        output_kaldi = !online2-wav-nnet3-latgen-faster \\\n",
    "        --online=false \\\n",
    "        --do-endpointing=false \\\n",
    "          --frame-subsampling-factor=3 \\\n",
    "          --config=exp/tdnn_7b_chain_online/conf/online.conf \\\n",
    "          --max-active=7000 \\\n",
    "          --beam=15.0 \\\n",
    "          --lattice-beam=6.0 \\\n",
    "          --acoustic-scale=1.0 \\\n",
    "          --word-symbol-table=exp/tdnn_7b_chain_online/graph_pp/words.txt \\\n",
    "          exp/tdnn_7b_chain_online/final.mdl \\\n",
    "          exp/tdnn_7b_chain_online/graph_pp/HCLG.fst \\\n",
    "          'ark:echo utterance-id1 utterance-id1|' \\\n",
    "          'scp:echo utterance-id1 {file_name_wav}|' \\\n",
    "          'ark,t:{file_name_lat}'\n",
    "        \n",
    "        transcriptions[file_name] = output_kaldi[7][14:]\n",
    "        \n",
    "    except:\n",
    "        error_files.append(each_file)\n",
    "        print (\"Kaldi error for \" + each_file +  \".  Did you run cmd.sh and path.sh?\")\n",
    "\n",
    "with open('0_5000Questions.json', 'w') as fp:\n",
    "    json.dump(transcriptions, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ASR]",
   "language": "python",
   "name": "conda-env-ASR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
